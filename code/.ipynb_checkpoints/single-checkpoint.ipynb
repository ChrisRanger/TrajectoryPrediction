{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm_notebook\n",
    "import gc, psutil\n",
    "\n",
    "print(l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'data_path': '/home/yx/WSY/Prediction/datasets/lyft-motion-prediction-autonomous-vehicles',\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name': \"single_resnet50\",\n",
    "        'lr': 1e-4,\n",
    "        'weight_path': '',\n",
    "        'train': True,\n",
    "        'predict': False,\n",
    "    },\n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "    },\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4,\n",
    "    },\n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 128,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4,\n",
    "    },\n",
    "    'train_params': {\n",
    "        'steps': 200000,\n",
    "        'metrics_steps': 1000,\n",
    "        'update_steps': 1000,\n",
    "        'checkpoint_steps': 1000,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, h_dim=64, num_layers=1, dropout=0.0, v_dim=2):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.v_dim = v_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.encoder = nn.LSTM(\n",
    "            embedding_dim, h_dim, num_layers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(v_dim, embedding_dim)\n",
    "\n",
    "    def init_hidden(self, batch):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda(),\n",
    "            torch.zeros(self.num_layers, batch, self.h_dim).cuda()\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_traj):\n",
    "        # obs_traj: history_len * batch * input_size\n",
    "        batch = obs_traj.size(1)\n",
    "        # 历史轨迹重构为(obs_len * batch, v_dim)并送入embedding层(全连接)，成为\n",
    "        # print(obs_traj.shape)\n",
    "        obs_traj_embedding = self.spatial_embedding(obs_traj.contiguous().view(-1, self.v_dim))\n",
    "        obs_traj_embedding = obs_traj_embedding.view(\n",
    "            -1, batch, self.embedding_dim\n",
    "        )\n",
    "        state_tuple = self.init_hidden(batch)\n",
    "        output, state = self.encoder(obs_traj_embedding, state_tuple)\n",
    "        final_h = state[0]\n",
    "        # obs_traj: batch * h_dim\n",
    "        return final_h\n",
    "\n",
    "\n",
    "class Single(nn.Module):\n",
    "    def __init__(self, cfg: Dict, num_modes=3, h_dim: int = 64):\n",
    "        super().__init__()\n",
    "        # encoder参数默认\n",
    "        self.encoder = Encoder(v_dim=4)\n",
    "        architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
    "        backbone = eval(architecture)(pretrained=True, progress=True)\n",
    "        self.backbone = backbone\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        if architecture == \"resnet50\":\n",
    "            backbone_out_features = 2048\n",
    "        else:\n",
    "            backbone_out_features = 512\n",
    "        \n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        num_targets = 2 * self.future_len\n",
    "        # 全连接层\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features+h_dim, out_features=4096),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # 输出层：输入全连接层的输出, 得到轨迹和概率\n",
    "        self.logit1 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.logit2 = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "        \n",
    "        # 全连接BN层\n",
    "        self.bn1 = nn.BatchNorm1d(4096)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "\n",
    "    def forward(self, x, history_traj):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # h: batch * h_dim\n",
    "        h = self.encoder(history_traj)\n",
    "\n",
    "        x = torch.cat([x, h.squeeze(0)], dim=1)\n",
    "        x = self.bn1(self.head(x))\n",
    "        x = self.bn2(self.logit1(x))\n",
    "        x = F.relu(self.logit2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion=nn.MSELoss(reduction=\"none\")):\n",
    "    inputs = data[\"image\"].to(device)\n",
    "    history_traj = torch.from_numpy(np.flip(data[\"history_positions\"].numpy(), axis=1))\n",
    "    history_yaw = torch.from_numpy(np.flip(data[\"history_yaws\"].numpy(), axis=1))\n",
    "    history_availabilities = torch.unsqueeze(torch.from_numpy(np.flip(data[\"history_availabilities\"].numpy(), axis=1)), 2)\n",
    "    # print(history_traj.shape, history_yaw.shape, history_availabilities.shape)\n",
    "    history = torch.cat([history_traj, history_yaw], 2)\n",
    "    history = torch.cat([history, history_availabilities], 2).to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs, history.permute(1, 0, 2)).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+ 22496709\n",
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT = cfg[\"data_path\"]\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager()\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open(cached=False)  # to prevent run out of memory\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"], num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset, len(train_dataset))\n",
    "\n",
    "train_writer = SummaryWriter('../log/Single', comment='Single')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Single(cfg)\n",
    "weight_path = cfg[\"model_params\"][\"weight_path\"]\n",
    "\n",
    "if weight_path:\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    print(weight_path, \"loaded\")\n",
    "model.to(device)\n",
    "learning_rate = cfg[\"model_params\"][\"lr\"]\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=10000,gamma = 0.5)\n",
    "print(f'device {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb081eb91cc4e59b5bfae1a379fc31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-96ab07f7b94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# somehow we need this is ever batch or it perform very bad (not sure why)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-976d73ac36fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(data, model, device, criterion)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory_traj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"history_positions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mhistory_yaw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"history_yaws\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhistory_availabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"history_availabilities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mflip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/wsy/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(m, axis)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "if cfg[\"model_params\"][\"train\"]:\n",
    "    tr_it = iter(train_dataloader)\n",
    "    n_steps = cfg[\"train_params\"][\"steps\"]\n",
    "    progress_bar = tqdm_notebook(range(1, 1 + n_steps), mininterval=5.)\n",
    "    losses = []\n",
    "    losses_fde = []\n",
    "    iterations = []\n",
    "    metrics = []\n",
    "    metrics_fde = []\n",
    "    times = []\n",
    "    model_name = cfg[\"model_params\"][\"model_name\"]\n",
    "    update_steps = cfg['train_params']['update_steps']\n",
    "    metrics_steps = cfg['train_params']['metrics_steps']\n",
    "    checkpoint_steps = cfg['train_params']['checkpoint_steps']\n",
    "    t_start = time.time()\n",
    "    torch.set_grad_enabled(True)\n",
    "    print('start train')\n",
    "\n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "        model.train()  # somehow we need this is ever batch or it perform very bad (not sure why)\n",
    "        loss, pred = forward(data, model, device)\n",
    "\n",
    "        # Backward pass\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_v = loss.item()\n",
    "        losses.append(loss_v)\n",
    "        train_writer.add_scalar('loss_v', loss_v, i)\n",
    "\n",
    "        if i % metrics_steps == 0:\n",
    "            \n",
    "            # 求其他尺度指标\n",
    "            final_target = data['target_positions'][:, -1, :].to(device)\n",
    "            fde_bs = []\n",
    "            for index in range(0, cfg['train_data_loader']['batch_size']):\n",
    "                fde = ((pred[index][-1][0] - final_target[index][0])**2 +\n",
    "                       (pred[index][-1][1] - final_target[index][1])**2)**0.5\n",
    "                fde_bs.append(fde.item())\n",
    "            losses_fde.append(np.mean(fde_bs))\n",
    "            train_writer.add_scalar('loss_fde', np.mean(fde_bs), i)\n",
    "\n",
    "\n",
    "        if i % update_steps == 0:\n",
    "            mean_losses = np.mean(losses)\n",
    "            losses = []\n",
    "            train_writer.add_scalar('mean_loss', mean_losses, i)\n",
    "            mean_fde = np.mean(losses_fde)\n",
    "            train_writer.add_scalar('mean_fde', mean_fde, i)\n",
    "            losses_fde = []\n",
    "            timespent = (time.time() - t_start) / 60\n",
    "            print('i: %5d' % i,\n",
    "                  'loss: %10.5f' % loss_v, 'loss(avg): %10.5f' % mean_losses,\n",
    "                  'loss_fde(avg): %10.5f' % mean_fde, ' %.2fmins' % timespent, end=' | \\n')\n",
    "            if i % checkpoint_steps == 0:\n",
    "                torch.save(model.state_dict(), f'../model/{model_name}.pth')\n",
    "                torch.save(optimizer.state_dict(), f'../model/{model_name}_optimizer.pth')\n",
    "            iterations.append(i)\n",
    "            metrics.append(mean_losses)\n",
    "            metrics_fde.append(mean_fde)\n",
    "            times.append(timespent)\n",
    "\n",
    "    torch.save(model.state_dict(), f'{model_name}_final.pth')\n",
    "    torch.save(optimizer.state_dict(), f'{model_name}_optimizer_final.pth')\n",
    "    results = pd.DataFrame({\n",
    "        'iterations': iterations,\n",
    "        'metrics (avg)': metrics,\n",
    "        'metrics_fde (avg)': metrics_fde,\n",
    "        'elapsed_time (mins)': times,\n",
    "    })\n",
    "    results.to_csv(f'train_metrics_{model_name}_{n_steps}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
