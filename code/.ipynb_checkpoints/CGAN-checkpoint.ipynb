{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "import utils\n",
    "import cgan\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm_notebook\n",
    "import gc, psutil\n",
    "\n",
    "print(l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'data_path': '/home/chris/predict_code/Prediction/datasets/lyft-motion-prediction-autonomous-vehicles',\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet34',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1,\n",
    "        'model_name_g': \"CGAN_resnet34_G\",\n",
    "        'model_name_d': \"CGAN_resnet34_D\",\n",
    "        'lr_g': 1e-3,\n",
    "        'lr_d': 1e-3,\n",
    "        'weight_path_g': '../model/CGAN_resnet34_G.pth',\n",
    "        'weight_path_d': '../model/CGAN_resnet34_D.pth',\n",
    "        'train': True,\n",
    "        'predict': False,\n",
    "    },\n",
    "    'raster_params': {\n",
    "        'raster_size': [400, 400],\n",
    "        'pixel_size': [0.3, 0.3],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "    },\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 16,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4,\n",
    "    },\n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 128,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4,\n",
    "    },\n",
    "    'train_params': {\n",
    "        'multi': 'best',\n",
    "        'steps': 200000,\n",
    "        'update_steps': 1000,\n",
    "        'metrics_steps': 100,\n",
    "        'checkpoint_steps': 2000,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+ 22496709\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集，准备device\n",
    "DIR_INPUT = cfg[\"data_path\"]\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager()\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open(cached=False)  # to prevent run out of memory\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"],\n",
    "                              batch_size=train_cfg[\"batch_size\"], num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset, len(train_dataset))\n",
    "\n",
    "train_writer = SummaryWriter('../log/CGAN', comment='CGAN')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/CGAN_resnet34_G.pth loaded\n",
      "../model/CGAN_resnet34_D.pth loaded\n",
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 建立模型\n",
    "generator = cgan.generator(cfg, traj_dim=256, cont_dim=256, mode=3)\n",
    "discriminator = cgan.discriminator(cfg, h_dim=256, cont_dim=256)\n",
    "weight_path_g = cfg[\"model_params\"][\"weight_path_g\"]\n",
    "if weight_path_g:\n",
    "    generator.load_state_dict(torch.load(weight_path_g))\n",
    "    print(weight_path_g, 'loaded')\n",
    "weight_path_d = cfg[\"model_params\"][\"weight_path_d\"]\n",
    "if weight_path_d:\n",
    "    discriminator.load_state_dict(torch.load(weight_path_d))\n",
    "    print(weight_path_d, 'loaded')\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "g_loss_fn = utils.g_loss\n",
    "d_loss_fn = utils.d_loss\n",
    "\n",
    "learning_rate_g = cfg[\"model_params\"][\"lr_g\"]\n",
    "learning_rate_d = cfg[\"model_params\"][\"lr_d\"]\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate_g)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate_d)\n",
    "\n",
    "scheduler_g = optim.lr_scheduler.StepLR(optimizer_g,step_size=30000,gamma = 0.1)\n",
    "scheduler_d = optim.lr_scheduler.StepLR(optimizer_d,step_size=30000,gamma = 0.1)\n",
    "print(f'device {device}')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器训练\n",
    "def forward_g(data, model_g, model_d, device, optimizer, scheduler, criterion=utils.g_loss, omega=20.0):\n",
    "    image = data[\"image\"].to(device)\n",
    "    history_traj = data[\"history_positions\"].flip(1)\n",
    "    history_yaw = data[\"history_yaws\"].flip(1)\n",
    "    history_availabilities = torch.unsqueeze(data[\"history_availabilities\"].flip(1), 2)\n",
    "    history = torch.cat([history_traj, history_yaw], 2)\n",
    "    history = torch.cat([history, history_availabilities], 2).to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    \n",
    "    model_g.train()\n",
    "    preds, confidences, condition = model_g(image, history.permute(1, 0, 2))\n",
    "    traj_fake = multi2single(preds, targets, target_availabilities, confidences, mode='best')\n",
    "    score_fake = model_d(traj_fake.permute(1, 0, 2), condition)\n",
    "    # 判别loss + nll_loss\n",
    "    g_loss = criterion(score_fake)\n",
    "    nll_loss = utils.pytorch_neg_multi_log_likelihood_batch(targets, preds,\n",
    "                                                            confidences, target_availabilities)\n",
    "#     loss = g_loss * omega + nll_loss\n",
    "    loss = g_loss\n",
    "    scheduler.step()        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, nll_loss, preds, confidences\n",
    "\n",
    "# 判别器训练\n",
    "def forward_d(data, model_g, model_d, device, optimizer, scheduler, criterion=utils.d_loss):\n",
    "    image = data[\"image\"].to(device)\n",
    "    history_traj = data[\"history_positions\"].flip(1)\n",
    "    history_yaw = data[\"history_yaws\"].flip(1)\n",
    "    history_availabilities = torch.unsqueeze(data[\"history_availabilities\"].flip(1), 2)\n",
    "    history = torch.cat([history_traj, history_yaw], 2)\n",
    "    history = torch.cat([history, history_availabilities], 2).to(device)\n",
    "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    \n",
    "    model_d.train()\n",
    "    preds, confidences, condition = model_g(image, history.permute(1, 0, 2))\n",
    "    traj_fake = multi2single(preds, targets, target_availabilities, confidences, mode='best')\n",
    "    score_fake = model_d(traj_fake.permute(1, 0, 2), condition)\n",
    "    score_real = model_d(targets.permute(1, 0, 2), condition)\n",
    "    loss = criterion(score_real, score_fake)\n",
    "    scheduler.step()        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# 多模态轨迹挑选函数，支持多种模态捕捉方式\n",
    "def multi2single(traj_multi, ground_truth, avails, confidence, mode='best'):\n",
    "    # 三种模式： best最接近gt那条; mean求期望; most取概率最大那条，其他方式待补充\n",
    "    if mode == 'most':\n",
    "        index = confidence.argmax(dim=1)\n",
    "        selected_traj = gt\n",
    "        for i in range(0, ground_truth.shape[0]):\n",
    "            selected_traj[i] = traj_multi[i][index[i]]\n",
    "        selected_traj = selected_traj.squeeze(1)\n",
    "        return selected_traj\n",
    "    elif mode == 'mean':\n",
    "        traj_multi[:][0] = traj_multi[:][0] * confidence[0]\n",
    "        traj_multi[:][1] = traj_multi[:][1] * confidence[1]\n",
    "        traj_multi[:][2] = traj_multi[:][2] * confidence[2]\n",
    "        traj_multi = torch.sum(traj_multi, dim=1)\n",
    "        return traj_multi/3.0\n",
    "    elif mode == 'best':\n",
    "        gt = torch.unsqueeze(ground_truth, 1)\n",
    "        avails = avails[:, None, :, None]\n",
    "        # 求L2距离平方\n",
    "        error = torch.sum(((gt - traj_multi) * avails) ** 2, dim=3)\n",
    "        # 时序轴上求平均\n",
    "        error = torch.mean(error, dim=2)\n",
    "        # 模态轴上求最小值\n",
    "        index = error.argmin(dim=1)\n",
    "        selected_traj = torch.tensor(gt)\n",
    "        for i in range(0, ground_truth.shape[0]):\n",
    "            selected_traj[i] = traj_multi[i][index[i]]\n",
    "        selected_traj = selected_traj.squeeze(1)\n",
    "        return selected_traj\n",
    "    else:\n",
    "        print('error, non-exist mode for select traj from multi!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf01b084a5d411bb3ef2f14f5a0720e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train, mode is  best\n",
      "i:  1000 loss_g(avg):    1.38088 loss_nll(avg): 158110.51095 loss_d(avg):    1.30036 loss_ade(avg):   44.46008 loss_fde(avg):   77.40488 lr(g): 0.001000 lr(d): 0.001000  26.11mins | \n",
      "i:  2000 loss_g(avg):    1.49790 loss_nll(avg): 562172.32901 loss_d(avg):    1.33727 loss_ade(avg):   98.57026 loss_fde(avg):  232.62565 lr(g): 0.001000 lr(d): 0.001000  52.86mins | \n",
      "i:  3000 loss_g(avg):    1.61132 loss_nll(avg): 1232739.68956 loss_d(avg):    1.34774 loss_ade(avg):  141.11176 loss_fde(avg):  371.74043 lr(g): 0.001000 lr(d): 0.001000  79.68mins | \n",
      "i:  4000 loss_g(avg):    2.04062 loss_nll(avg): 1392645.36513 loss_d(avg):    1.32983 loss_ade(avg):  138.78735 loss_fde(avg):  410.85938 lr(g): 0.001000 lr(d): 0.001000  106.43mins | \n",
      "i:  5000 loss_g(avg):    1.78469 loss_nll(avg): 1334726.09963 loss_d(avg):    1.18038 loss_ade(avg):  125.53549 loss_fde(avg):  367.94632 lr(g): 0.001000 lr(d): 0.001000  132.36mins | \n",
      "i:  6000 loss_g(avg):    1.75751 loss_nll(avg): 1371062.44650 loss_d(avg):    1.25786 loss_ade(avg):  148.64230 loss_fde(avg):  457.99725 lr(g): 0.001000 lr(d): 0.001000  157.98mins | \n",
      "i:  7000 loss_g(avg):    1.82976 loss_nll(avg): 1819502.93184 loss_d(avg):    1.35921 loss_ade(avg):  167.16184 loss_fde(avg):  567.73354 lr(g): 0.001000 lr(d): 0.001000  183.31mins | \n",
      "i:  8000 loss_g(avg):    1.74003 loss_nll(avg): 2277001.52100 loss_d(avg):    1.27084 loss_ade(avg):  185.02599 loss_fde(avg):  513.57114 lr(g): 0.001000 lr(d): 0.001000  208.52mins | \n",
      "i:  9000 loss_g(avg):    2.05136 loss_nll(avg): 2731048.35988 loss_d(avg):    1.33421 loss_ade(avg):  211.06107 loss_fde(avg):  581.73244 lr(g): 0.001000 lr(d): 0.001000  233.56mins | \n",
      "i: 10000 loss_g(avg):    2.21670 loss_nll(avg): 2805448.08269 loss_d(avg):    1.48521 loss_ade(avg):  211.70274 loss_fde(avg):  576.78808 lr(g): 0.001000 lr(d): 0.001000  258.50mins | \n",
      "i: 11000 loss_g(avg):    2.24732 loss_nll(avg): 2832121.67238 loss_d(avg):    1.53183 loss_ade(avg):  214.52580 loss_fde(avg):  549.12770 lr(g): 0.001000 lr(d): 0.001000  283.26mins | \n",
      "i: 12000 loss_g(avg):    2.66860 loss_nll(avg): 3022921.58956 loss_d(avg):    1.64437 loss_ade(avg):  186.25615 loss_fde(avg):  488.12402 lr(g): 0.001000 lr(d): 0.001000  308.01mins | \n",
      "i: 13000 loss_g(avg):    2.41126 loss_nll(avg): 5939804.45463 loss_d(avg):    1.40755 loss_ade(avg):  276.76561 loss_fde(avg):  834.85811 lr(g): 0.001000 lr(d): 0.001000  332.75mins | \n",
      "i: 14000 loss_g(avg):    1.94348 loss_nll(avg): 6573649.82038 loss_d(avg):    1.35150 loss_ade(avg):  301.12481 loss_fde(avg):  949.65270 lr(g): 0.001000 lr(d): 0.001000  357.57mins | \n",
      "i: 15000 loss_g(avg):    1.87402 loss_nll(avg): 4889347.24419 loss_d(avg):    1.26650 loss_ade(avg):  225.56251 loss_fde(avg):  725.89769 lr(g): 0.001000 lr(d): 0.001000  382.32mins | \n",
      "i: 16000 loss_g(avg):    1.96301 loss_nll(avg): 4440382.72263 loss_d(avg):    1.44281 loss_ade(avg):  253.67178 loss_fde(avg):  749.41907 lr(g): 0.001000 lr(d): 0.001000  406.94mins | \n",
      "i: 17000 loss_g(avg):    2.90628 loss_nll(avg): 6904116.87762 loss_d(avg):    1.52084 loss_ade(avg):  297.82436 loss_fde(avg):  979.30294 lr(g): 0.001000 lr(d): 0.001000  431.56mins | \n",
      "i: 18000 loss_g(avg):    2.43407 loss_nll(avg): 10039696.81450 loss_d(avg):    1.47908 loss_ade(avg):  389.00288 loss_fde(avg): 1588.07177 lr(g): 0.001000 lr(d): 0.001000  456.12mins | \n",
      "i: 19000 loss_g(avg):    2.34417 loss_nll(avg): 11946575.08925 loss_d(avg):    1.51268 loss_ade(avg):  363.28362 loss_fde(avg): 1206.71211 lr(g): 0.001000 lr(d): 0.001000  480.59mins | \n",
      "i: 20000 loss_g(avg):    2.24611 loss_nll(avg): 12345264.50000 loss_d(avg):    1.31938 loss_ade(avg):  408.82040 loss_fde(avg): 1275.33828 lr(g): 0.001000 lr(d): 0.001000  505.11mins | \n",
      "i: 21000 loss_g(avg):    3.04022 loss_nll(avg): 13511423.23500 loss_d(avg):    1.45710 loss_ade(avg):  464.55559 loss_fde(avg): 1133.44026 lr(g): 0.001000 lr(d): 0.001000  529.65mins | \n",
      "i: 22000 loss_g(avg):    2.35032 loss_nll(avg): 15158205.32625 loss_d(avg):    1.20768 loss_ade(avg):  435.03653 loss_fde(avg): 1047.77376 lr(g): 0.001000 lr(d): 0.001000  554.26mins | \n",
      "i: 23000 loss_g(avg):    2.61426 loss_nll(avg): 18241214.03800 loss_d(avg):    1.64913 loss_ade(avg):  485.57074 loss_fde(avg): 1249.54877 lr(g): 0.001000 lr(d): 0.001000  578.86mins | \n",
      "i: 24000 loss_g(avg):    2.57921 loss_nll(avg): 15029981.04425 loss_d(avg):    1.36830 loss_ade(avg):  437.07787 loss_fde(avg): 1363.27537 lr(g): 0.001000 lr(d): 0.001000  603.55mins | \n",
      "i: 25000 loss_g(avg):    2.21960 loss_nll(avg): 13916311.52500 loss_d(avg):    1.15049 loss_ade(avg):  435.88085 loss_fde(avg): 1370.54025 lr(g): 0.001000 lr(d): 0.001000  628.28mins | \n",
      "i: 26000 loss_g(avg):    2.96257 loss_nll(avg): 10600184.32638 loss_d(avg):    1.79673 loss_ade(avg):  353.55954 loss_fde(avg): 1088.88757 lr(g): 0.001000 lr(d): 0.001000  652.76mins | \n",
      "i: 27000 loss_g(avg):    2.32906 loss_nll(avg): 10501078.88775 loss_d(avg):    1.52773 loss_ade(avg):  365.84271 loss_fde(avg): 1183.42556 lr(g): 0.001000 lr(d): 0.001000  677.31mins | \n",
      "i: 28000 loss_g(avg):    2.54950 loss_nll(avg): 10697791.94075 loss_d(avg):    1.52806 loss_ade(avg):  314.99948 loss_fde(avg):  992.54370 lr(g): 0.001000 lr(d): 0.001000  701.91mins | \n",
      "i: 29000 loss_g(avg):    2.65973 loss_nll(avg): 10708252.37438 loss_d(avg):    1.54000 loss_ade(avg):  388.77728 loss_fde(avg): 1370.18319 lr(g): 0.001000 lr(d): 0.001000  726.14mins | \n",
      "i: 30000 loss_g(avg):    2.36349 loss_nll(avg): 13503600.93075 loss_d(avg):    1.30308 loss_ade(avg):  464.74223 loss_fde(avg): 1758.12623 lr(g): 0.000100 lr(d): 0.000100  749.70mins | \n",
      "i: 31000 loss_g(avg):    1.87869 loss_nll(avg): 15141577.30250 loss_d(avg):    0.86542 loss_ade(avg):  426.80366 loss_fde(avg): 1384.29622 lr(g): 0.000100 lr(d): 0.000100  773.35mins | \n",
      "i: 32000 loss_g(avg):    1.86630 loss_nll(avg): 15217142.48700 loss_d(avg):    0.78514 loss_ade(avg):  427.96447 loss_fde(avg): 1351.32773 lr(g): 0.000100 lr(d): 0.000100  796.97mins | \n",
      "i: 33000 loss_g(avg):    1.89558 loss_nll(avg): 15803841.57000 loss_d(avg):    0.83268 loss_ade(avg):  431.61797 loss_fde(avg): 1485.28783 lr(g): 0.000100 lr(d): 0.000100  820.60mins | \n",
      "i: 34000 loss_g(avg):    2.04057 loss_nll(avg): 16764414.81550 loss_d(avg):    0.95303 loss_ade(avg):  453.95056 loss_fde(avg): 1590.53990 lr(g): 0.000100 lr(d): 0.000100  844.26mins | \n",
      "i: 35000 loss_g(avg):    1.89464 loss_nll(avg): 16911675.63400 loss_d(avg):    0.87056 loss_ade(avg):  471.93559 loss_fde(avg): 1570.16163 lr(g): 0.000100 lr(d): 0.000100  868.44mins | \n",
      "i: 36000 loss_g(avg):    2.02155 loss_nll(avg): 16665489.50100 loss_d(avg):    0.85996 loss_ade(avg):  504.88861 loss_fde(avg): 1689.97435 lr(g): 0.000100 lr(d): 0.000100  892.64mins | \n",
      "i: 37000 loss_g(avg):    2.13622 loss_nll(avg): 16759737.72400 loss_d(avg):    0.97074 loss_ade(avg):  446.24819 loss_fde(avg): 1441.49926 lr(g): 0.000100 lr(d): 0.000100  917.80mins | \n",
      "i: 38000 loss_g(avg):    2.09071 loss_nll(avg): 16574622.15800 loss_d(avg):    0.93203 loss_ade(avg):  439.13150 loss_fde(avg): 1433.76548 lr(g): 0.000100 lr(d): 0.000100  942.69mins | \n",
      "i: 39000 loss_g(avg):    2.11883 loss_nll(avg): 16671510.75150 loss_d(avg):    1.09953 loss_ade(avg):  451.83163 loss_fde(avg): 1514.63073 lr(g): 0.000100 lr(d): 0.000100  967.91mins | \n",
      "i: 40000 loss_g(avg):    2.14790 loss_nll(avg): 16755100.24250 loss_d(avg):    1.03454 loss_ade(avg):  472.12969 loss_fde(avg): 1534.62195 lr(g): 0.000100 lr(d): 0.000100  993.15mins | \n",
      "i: 41000 loss_g(avg):    2.15214 loss_nll(avg): 16538398.80725 loss_d(avg):    1.01272 loss_ade(avg):  457.80847 loss_fde(avg): 1521.21099 lr(g): 0.000100 lr(d): 0.000100  1018.43mins | \n",
      "i: 42000 loss_g(avg):    2.04348 loss_nll(avg): 16557178.85250 loss_d(avg):    1.03479 loss_ade(avg):  475.38385 loss_fde(avg): 1588.48933 lr(g): 0.000100 lr(d): 0.000100  1043.83mins | \n",
      "i: 43000 loss_g(avg):    2.06587 loss_nll(avg): 16327528.46950 loss_d(avg):    0.93656 loss_ade(avg):  495.99973 loss_fde(avg): 1694.93446 lr(g): 0.000100 lr(d): 0.000100  1068.80mins | \n",
      "i: 44000 loss_g(avg):    2.09747 loss_nll(avg): 16335942.26225 loss_d(avg):    0.97854 loss_ade(avg):  474.94478 loss_fde(avg): 1574.77212 lr(g): 0.000100 lr(d): 0.000100  1093.57mins | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 45000 loss_g(avg):    2.05257 loss_nll(avg): 15647719.09175 loss_d(avg):    1.03826 loss_ade(avg):  423.89262 loss_fde(avg): 1423.21404 lr(g): 0.000100 lr(d): 0.000100  1121.48mins | \n",
      "i: 46000 loss_g(avg):    2.18976 loss_nll(avg): 15746052.30125 loss_d(avg):    1.02660 loss_ade(avg):  473.28074 loss_fde(avg): 1604.18236 lr(g): 0.000100 lr(d): 0.000100  1149.90mins | \n",
      "i: 47000 loss_g(avg):    2.16125 loss_nll(avg): 16563188.63800 loss_d(avg):    1.00803 loss_ade(avg):  482.54236 loss_fde(avg): 1606.41786 lr(g): 0.000100 lr(d): 0.000100  1178.35mins | \n",
      "i: 48000 loss_g(avg):    2.16920 loss_nll(avg): 16149726.17000 loss_d(avg):    1.03040 loss_ade(avg):  452.81593 loss_fde(avg): 1448.15026 lr(g): 0.000100 lr(d): 0.000100  1206.06mins | \n",
      "i: 49000 loss_g(avg):    2.02954 loss_nll(avg): 17341892.57650 loss_d(avg):    0.96791 loss_ade(avg):  467.07235 loss_fde(avg): 1383.71253 lr(g): 0.000100 lr(d): 0.000100  1232.77mins | \n",
      "i: 50000 loss_g(avg):    2.18917 loss_nll(avg): 17476261.49450 loss_d(avg):    1.17238 loss_ade(avg):  481.96883 loss_fde(avg): 1571.55897 lr(g): 0.000100 lr(d): 0.000100  1259.87mins | \n",
      "i: 51000 loss_g(avg):    2.19536 loss_nll(avg): 17590934.05300 loss_d(avg):    1.07756 loss_ade(avg):  488.93425 loss_fde(avg): 1518.61487 lr(g): 0.000100 lr(d): 0.000100  1287.08mins | \n",
      "i: 52000 loss_g(avg):    2.20481 loss_nll(avg): 17375848.02900 loss_d(avg):    1.10774 loss_ade(avg):  463.48005 loss_fde(avg): 1487.38723 lr(g): 0.000100 lr(d): 0.000100  1313.42mins | \n",
      "i: 53000 loss_g(avg):    2.17622 loss_nll(avg): 17650923.46450 loss_d(avg):    1.12400 loss_ade(avg):  479.96490 loss_fde(avg): 1553.74521 lr(g): 0.000100 lr(d): 0.000100  1339.53mins | \n",
      "i: 54000 loss_g(avg):    2.19534 loss_nll(avg): 17075860.42550 loss_d(avg):    1.15305 loss_ade(avg):  491.00330 loss_fde(avg): 1560.47513 lr(g): 0.000100 lr(d): 0.000100  1365.82mins | \n",
      "i: 55000 loss_g(avg):    2.25001 loss_nll(avg): 17396644.67650 loss_d(avg):    1.26210 loss_ade(avg):  480.09483 loss_fde(avg): 1521.84260 lr(g): 0.000100 lr(d): 0.000100  1391.44mins | \n",
      "i: 56000 loss_g(avg):    2.25215 loss_nll(avg): 18107997.50650 loss_d(avg):    1.23846 loss_ade(avg):  476.87757 loss_fde(avg): 1499.05969 lr(g): 0.000100 lr(d): 0.000100  1416.11mins | \n",
      "i: 57000 loss_g(avg):    2.20143 loss_nll(avg): 17741128.72650 loss_d(avg):    1.02194 loss_ade(avg):  478.32053 loss_fde(avg): 1445.74742 lr(g): 0.000100 lr(d): 0.000100  1441.14mins | \n",
      "i: 58000 loss_g(avg):    2.22341 loss_nll(avg): 17081647.43125 loss_d(avg):    1.33445 loss_ade(avg):  513.32406 loss_fde(avg): 1690.70599 lr(g): 0.000100 lr(d): 0.000100  1466.34mins | \n",
      "i: 59000 loss_g(avg):    2.13953 loss_nll(avg): 17629410.23675 loss_d(avg):    0.97684 loss_ade(avg):  470.85225 loss_fde(avg): 1449.14521 lr(g): 0.000100 lr(d): 0.000100  1491.76mins | \n",
      "i: 60000 loss_g(avg):    2.14541 loss_nll(avg): 17522011.98650 loss_d(avg):    1.06345 loss_ade(avg):  488.89533 loss_fde(avg): 1450.38257 lr(g): 0.000010 lr(d): 0.000010  1517.04mins | \n",
      "i: 61000 loss_g(avg):    1.94092 loss_nll(avg): 18182530.14350 loss_d(avg):    0.61238 loss_ade(avg):  476.64722 loss_fde(avg): 1474.86652 lr(g): 0.000010 lr(d): 0.000010  1541.79mins | \n",
      "i: 62000 loss_g(avg):    1.90830 loss_nll(avg): 17939031.37725 loss_d(avg):    0.48222 loss_ade(avg):  519.47645 loss_fde(avg): 1639.90577 lr(g): 0.000010 lr(d): 0.000010  1566.40mins | \n",
      "i: 63000 loss_g(avg):    1.92933 loss_nll(avg): 17939461.49600 loss_d(avg):    0.43787 loss_ade(avg):  496.06258 loss_fde(avg): 1480.83448 lr(g): 0.000010 lr(d): 0.000010  1590.73mins | \n",
      "i: 64000 loss_g(avg):    1.95021 loss_nll(avg): 17857671.33450 loss_d(avg):    0.42447 loss_ade(avg):  505.24290 loss_fde(avg): 1489.92052 lr(g): 0.000010 lr(d): 0.000010  1614.68mins | \n",
      "i: 65000 loss_g(avg):    1.96503 loss_nll(avg): 18122548.50800 loss_d(avg):    0.44554 loss_ade(avg):  487.03658 loss_fde(avg): 1497.79689 lr(g): 0.000010 lr(d): 0.000010  1638.85mins | \n",
      "i: 66000 loss_g(avg):    1.97959 loss_nll(avg): 17973758.64750 loss_d(avg):    0.48621 loss_ade(avg):  529.32551 loss_fde(avg): 1604.85624 lr(g): 0.000010 lr(d): 0.000010  1662.91mins | \n",
      "i: 67000 loss_g(avg):    1.99935 loss_nll(avg): 18196333.87150 loss_d(avg):    0.45992 loss_ade(avg):  526.61639 loss_fde(avg): 1575.86528 lr(g): 0.000010 lr(d): 0.000010  1687.21mins | \n",
      "i: 68000 loss_g(avg):    1.99733 loss_nll(avg): 18488722.76850 loss_d(avg):    0.45048 loss_ade(avg):  491.77131 loss_fde(avg): 1437.76608 lr(g): 0.000010 lr(d): 0.000010  1711.18mins | \n",
      "i: 69000 loss_g(avg):    2.02105 loss_nll(avg): 18070746.51000 loss_d(avg):    0.45024 loss_ade(avg):  498.74343 loss_fde(avg): 1565.02668 lr(g): 0.000010 lr(d): 0.000010  1735.07mins | \n",
      "i: 70000 loss_g(avg):    2.03905 loss_nll(avg): 18235624.69350 loss_d(avg):    0.42618 loss_ade(avg):  515.84437 loss_fde(avg): 1682.69534 lr(g): 0.000010 lr(d): 0.000010  1758.90mins | \n",
      "i: 71000 loss_g(avg):    2.03581 loss_nll(avg): 18182449.91275 loss_d(avg):    0.42008 loss_ade(avg):  512.96120 loss_fde(avg): 1566.50790 lr(g): 0.000010 lr(d): 0.000010  1782.75mins | \n",
      "i: 72000 loss_g(avg):    2.07186 loss_nll(avg): 18613136.91500 loss_d(avg):    0.44093 loss_ade(avg):  537.23772 loss_fde(avg): 1683.10381 lr(g): 0.000010 lr(d): 0.000010  1806.41mins | \n",
      "i: 73000 loss_g(avg):    2.06654 loss_nll(avg): 18560193.86650 loss_d(avg):    0.43649 loss_ade(avg):  554.96400 loss_fde(avg): 1732.67815 lr(g): 0.000010 lr(d): 0.000010  1830.22mins | \n",
      "i: 74000 loss_g(avg):    2.08684 loss_nll(avg): 18247134.87900 loss_d(avg):    0.42279 loss_ade(avg):  533.62133 loss_fde(avg): 1640.65736 lr(g): 0.000010 lr(d): 0.000010  1853.92mins | \n",
      "i: 75000 loss_g(avg):    2.10006 loss_nll(avg): 18461416.29150 loss_d(avg):    0.43148 loss_ade(avg):  526.27409 loss_fde(avg): 1618.37584 lr(g): 0.000010 lr(d): 0.000010  1877.70mins | \n",
      "i: 76000 loss_g(avg):    2.08396 loss_nll(avg): 18241091.04725 loss_d(avg):    0.38691 loss_ade(avg):  522.46096 loss_fde(avg): 1549.99001 lr(g): 0.000010 lr(d): 0.000010  1901.38mins | \n",
      "i: 77000 loss_g(avg):    2.10875 loss_nll(avg): 18343264.17100 loss_d(avg):    0.49515 loss_ade(avg):  470.82503 loss_fde(avg): 1247.51724 lr(g): 0.000010 lr(d): 0.000010  1925.17mins | \n",
      "i: 78000 loss_g(avg):    2.09256 loss_nll(avg): 18366308.95800 loss_d(avg):    0.42088 loss_ade(avg):  541.30178 loss_fde(avg): 1650.00110 lr(g): 0.000010 lr(d): 0.000010  1948.88mins | \n",
      "i: 79000 loss_g(avg):    2.12547 loss_nll(avg): 18699684.74100 loss_d(avg):    0.43441 loss_ade(avg):  517.16884 loss_fde(avg): 1556.99298 lr(g): 0.000010 lr(d): 0.000010  1972.72mins | \n",
      "i: 80000 loss_g(avg):    2.11660 loss_nll(avg): 18722377.72250 loss_d(avg):    0.42093 loss_ade(avg):  539.39064 loss_fde(avg): 1668.95317 lr(g): 0.000010 lr(d): 0.000010  1996.42mins | \n",
      "i: 81000 loss_g(avg):    2.14133 loss_nll(avg): 18577538.74100 loss_d(avg):    0.49212 loss_ade(avg):  536.48857 loss_fde(avg): 1706.40712 lr(g): 0.000010 lr(d): 0.000010  2020.14mins | \n",
      "i: 82000 loss_g(avg):    2.12906 loss_nll(avg): 18683229.47450 loss_d(avg):    0.40541 loss_ade(avg):  531.37468 loss_fde(avg): 1565.04149 lr(g): 0.000010 lr(d): 0.000010  2043.91mins | \n",
      "i: 83000 loss_g(avg):    2.14226 loss_nll(avg): 18634011.64350 loss_d(avg):    0.42770 loss_ade(avg):  531.62132 loss_fde(avg): 1650.02728 lr(g): 0.000010 lr(d): 0.000010  2067.73mins | \n",
      "i: 84000 loss_g(avg):    2.14381 loss_nll(avg): 18436520.44400 loss_d(avg):    0.46644 loss_ade(avg):  484.68707 loss_fde(avg): 1484.10634 lr(g): 0.000010 lr(d): 0.000010  2091.43mins | \n",
      "i: 85000 loss_g(avg):    2.14910 loss_nll(avg): 18640738.44350 loss_d(avg):    0.42252 loss_ade(avg):  490.27222 loss_fde(avg): 1460.33488 lr(g): 0.000010 lr(d): 0.000010  2115.20mins | \n",
      "i: 86000 loss_g(avg):    2.15906 loss_nll(avg): 18522240.20950 loss_d(avg):    0.39621 loss_ade(avg):  521.74591 loss_fde(avg): 1563.94888 lr(g): 0.000010 lr(d): 0.000010  2138.94mins | \n",
      "i: 87000 loss_g(avg):    2.16347 loss_nll(avg): 18659114.76050 loss_d(avg):    0.44399 loss_ade(avg):  518.78376 loss_fde(avg): 1551.96709 lr(g): 0.000010 lr(d): 0.000010  2162.72mins | \n",
      "i: 88000 loss_g(avg):    2.15872 loss_nll(avg): 18678475.58450 loss_d(avg):    0.42345 loss_ade(avg):  517.56010 loss_fde(avg): 1501.73060 lr(g): 0.000010 lr(d): 0.000010  2186.39mins | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 89000 loss_g(avg):    2.16794 loss_nll(avg): 18547951.75950 loss_d(avg):    0.39268 loss_ade(avg):  539.22831 loss_fde(avg): 1686.16868 lr(g): 0.000010 lr(d): 0.000010  2210.16mins | \n",
      "i: 90000 loss_g(avg):    2.15898 loss_nll(avg): 18318459.29450 loss_d(avg):    0.41941 loss_ade(avg):  478.83927 loss_fde(avg): 1395.08315 lr(g): 0.000001 lr(d): 0.000001  2233.88mins | \n",
      "i: 91000 loss_g(avg):    2.17554 loss_nll(avg): 18442603.44725 loss_d(avg):    0.34268 loss_ade(avg):  523.79698 loss_fde(avg): 1512.87933 lr(g): 0.000001 lr(d): 0.000001  2257.81mins | \n",
      "i: 92000 loss_g(avg):    2.17726 loss_nll(avg): 18615112.05300 loss_d(avg):    0.32839 loss_ade(avg):  543.26805 loss_fde(avg): 1657.36787 lr(g): 0.000001 lr(d): 0.000001  2281.55mins | \n",
      "i: 93000 loss_g(avg):    2.17967 loss_nll(avg): 18672415.77550 loss_d(avg):    0.34043 loss_ade(avg):  544.94049 loss_fde(avg): 1689.11724 lr(g): 0.000001 lr(d): 0.000001  2305.44mins | \n",
      "i: 94000 loss_g(avg):    2.20433 loss_nll(avg): 18748333.89750 loss_d(avg):    0.32750 loss_ade(avg):  497.90131 loss_fde(avg): 1461.51929 lr(g): 0.000001 lr(d): 0.000001  2329.25mins | \n",
      "i: 95000 loss_g(avg):    2.22811 loss_nll(avg): 18357555.49250 loss_d(avg):    0.34398 loss_ade(avg):  530.77433 loss_fde(avg): 1633.58378 lr(g): 0.000001 lr(d): 0.000001  2353.14mins | \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1e2ef77a8672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                                \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_g\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                criterion=g_loss_fn, omega=1.0)\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mlosses_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss_g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "if cfg[\"model_params\"][\"train\"]:\n",
    "    tr_it = iter(train_dataloader)\n",
    "    n_steps = cfg[\"train_params\"][\"steps\"]\n",
    "    progress_bar = tqdm_notebook(range(1, 1 + n_steps), mininterval=5.)\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    losses_nll = []\n",
    "    losses_ade = []\n",
    "    losses_fde = []\n",
    "    iterations = []\n",
    "    metrics_g = []\n",
    "    metrics_d = []\n",
    "    metrics_nll = []\n",
    "    metrics_ade = []\n",
    "    metrics_fde = []\n",
    "    times = []\n",
    "    model_name_g = cfg[\"model_params\"][\"model_name_g\"]\n",
    "    model_name_d = cfg[\"model_params\"][\"model_name_d\"]\n",
    "    multi_mode = cfg[\"train_params\"][\"multi\"]\n",
    "    update_steps = cfg['train_params']['update_steps']\n",
    "    metrics_steps = cfg['train_params']['metrics_steps']\n",
    "    checkpoint_steps = cfg['train_params']['checkpoint_steps']\n",
    "    t_start = time.time()\n",
    "    torch.set_grad_enabled(True)\n",
    "    print('start train, mode is ', multi_mode)\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "        \n",
    "        # 判别器训练\n",
    "        loss_d = forward_d(data, generator, discriminator, device,\n",
    "                           optimizer_d, scheduler_d, criterion=d_loss_fn)\n",
    "        loss_d = loss_d.item()\n",
    "        losses_d.append(loss_d)\n",
    "        train_writer.add_scalar('loss_d', loss_d, i)\n",
    "        \n",
    "        # 生成器训练\n",
    "        loss_g, loss_nll, preds, confidences = forward_g(data, generator, discriminator,\n",
    "                                               device, optimizer_g, scheduler_g,\n",
    "                                               criterion=g_loss_fn, omega=1.0)\n",
    "        loss_g = loss_g.item()\n",
    "        losses_g.append(loss_g)\n",
    "        train_writer.add_scalar('loss_g', loss_g, i)\n",
    "        loss_nll = loss_nll.item()\n",
    "        losses_nll.append(loss_nll)\n",
    "        train_writer.add_scalar('loss_nll', loss_nll, i)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % metrics_steps == 0:\n",
    "            # 求其他尺度指标\n",
    "            loss_ade = utils._average_displacement_error(data[\"target_positions\"].to(device), \n",
    "                                                         preds, confidences,\n",
    "                                                         data[\"target_availabilities\"].to(device), \n",
    "                                                         mode=multi_mode)\n",
    "            loss_fde = utils._final_displacement_error(data[\"target_positions\"].to(device), \n",
    "                                                       preds, confidences,\n",
    "                                                       data[\"target_availabilities\"].to(device),\n",
    "                                                       mode=multi_mode)\n",
    "            \n",
    "            losses_ade.append(loss_ade.item())\n",
    "            losses_fde.append(loss_fde.item())\n",
    "            train_writer.add_scalar('loss_ade', loss_ade, i)\n",
    "            train_writer.add_scalar('loss_fde', loss_fde, i)\n",
    "\n",
    "        if i % update_steps == 0:\n",
    "            mean_losses_g = np.mean(losses_g)\n",
    "            mean_losses_d = np.mean(losses_d)\n",
    "            train_writer.add_scalar('mean_loss_g', mean_losses_g, i)\n",
    "            train_writer.add_scalar('mean_loss_d', mean_losses_d, i)\n",
    "            mean_losses_ade = np.mean(losses_ade)\n",
    "            mean_losses_fde = np.mean(losses_fde)\n",
    "            mean_losses_nll = np.mean(losses_nll)\n",
    "            losses_g = []\n",
    "            losses_d = []\n",
    "            losses_nll = []\n",
    "            losses_ade = []\n",
    "            losses_fde = []\n",
    "            train_writer.add_scalar('mean_losses_nll', mean_losses_nll, i)\n",
    "            train_writer.add_scalar('loss_ade', mean_losses_ade, i)\n",
    "            train_writer.add_scalar('loss_fde', mean_losses_fde, i)\n",
    "            timespent = (time.time() - t_start) / 60\n",
    "            curr_lr_g = optimizer_g.param_groups[0]['lr']\n",
    "            curr_lr_d = optimizer_d.param_groups[0]['lr']\n",
    "            print('i: %5d' % i,\n",
    "                  'loss_g(avg): %10.5f' % mean_losses_g, 'loss_nll(avg): %10.5f' % mean_losses_nll,\n",
    "                  'loss_d(avg): %10.5f' % mean_losses_d, 'loss_ade(avg): %10.5f' % mean_losses_ade,\n",
    "                  'loss_fde(avg): %10.5f' % mean_losses_fde,'lr(g): %f' %curr_lr_g, 'lr(d): %f' %curr_lr_d,\n",
    "                  ' %.2fmins' % timespent, end=' | \\n')\n",
    "            if i % checkpoint_steps == 0:\n",
    "                torch.save(generator, f'../model/{model_name_g}.pkl')\n",
    "                torch.save(generator.state_dict(), f'../model/{model_name_g}.pth')\n",
    "                torch.save(optimizer_g.state_dict(), f'../model/{model_name_g}_optimizer.pth')\n",
    "                torch.save(discriminator, f'../model/{model_name_d}.pkl')\n",
    "                torch.save(discriminator.state_dict(), f'../model/{model_name_d}.pth')\n",
    "                torch.save(optimizer_d.state_dict(), f'../model/{model_name_d}_optimizer.pth')\n",
    "            iterations.append(i)\n",
    "            metrics_g.append(mean_losses_g)\n",
    "            metrics_d.append(mean_losses_d)\n",
    "            metrics_ade.append(mean_losses_ade)\n",
    "            metrics_fde.append(mean_losses_fde)\n",
    "            metrics_nll.append(mean_losses_nll)\n",
    "            times.append(timespent)\n",
    "\n",
    "    torch.save(generator.state_dict(), f'../model/{model_name_g}_final.pth')\n",
    "    torch.save(generator, f'../model/{model_name_g}_final.pkl')\n",
    "    torch.save(optimizer_g.state_dict(), f'../model/{model_name_g}_optimizer_final.pth')\n",
    "    torch.save(discriminator, f'../model/{model_name_d}.pkl')\n",
    "    torch.save(discriminator.state_dict(), f'../model/{model_name_d}.pth')\n",
    "    torch.save(optimizer_d.state_dict(), f'../model/{model_name_d}_optimizer.pth')\n",
    "    results = pd.DataFrame({\n",
    "        'iterations': iterations,\n",
    "        'metrics_g (avg)': metrics_g,\n",
    "        'metrics_d (avg)': metrics_d,\n",
    "        'metrics_ade (avg)': metrics_ade,\n",
    "        'metrics_fde (avg)': metrics_fde,\n",
    "        'metrics_nll (avg)': metrics_nll,\n",
    "        'elapsed_time (mins)': times,\n",
    "    })\n",
    "    results.to_csv(f'../log/train_metrics_cgan_{n_steps}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
